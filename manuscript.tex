\documentclass{article}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Re-initializing k-means clustering with additional observations}
\author[1]{Aaron Lun}
\affil[1]{Genentech, Inc. South San Francisco, CA}

\begin{document}
\maketitle

\section{Motivation}

We consider the problem of k-means clustering \cite{lloyd1982least} after a batch of extra observations have been added to the original dataset.
More specifically, assume that the original dataset has already been subjected to k-means clustering.
After adding the extra observations, we wish to obtain a new clustering of the expanded dataset with the same $k$.
One obvious approach is to just assign each new observation to the closest existing cluster followed by post-initialization refinement - we will refer to this as the simple method.
The other obvious approach is to repeat the entire k-means procedure on the expanded dataset without using any information from the existing clustering - we will refer to this as a "fresh" clustering.

We would like our new clustering to account for new subpopulations among the recently added observations.
This ensures that our new clustering can capture subpopulations that were not observed in the original dataset,
which is useful when observations are coming in batches from multiple non-replicate sources.
Unfortunately, the simple method struggles with this requirement as it starts in a local minimum based on the existing cluster centers.
To escape, a new subpopulation must be large enough to relocate the center of the closest existing cluster, 
while hoping that the observations for that existing cluster can be sensibly reassigned to another existing cluster.

We would also like our new clustering to retain some similarity to the existing clustering of the original dataset.
This is useful when the existing clustering is known to be of interest - 
for example, a domain expert has manually generated annotations for each cluster,
and we want to preserve as much of this information as we can in our new clustering.
It can also be desirable when the existing clustering is high-quality but expensive, e.g., after long iteration times or multiple restarts,
and we would like to maintain quality without repeating the computational work.
Such preservation is difficult to achieve in a fresh clustering where non-obvious partitions may not be recapitulated.

\section{Proposed re-initialization algorithm}

To address these two concerns, we developed a algorithm for ``re-initialization" of a k-means clustering in an expanded dataset containing a batch of new observations.
For each existing cluster $i$, we imagine a scenario where $i$'s center is removed from the set of cluster centers.
We compute the squared distance of each observation to its closest remaining center.
We randomly sample a single observation where the sampling probability for each observation is proportional to its squared distance \cite{vassilvitskii2006kmeanspp}.
The sampled observation is our proposal for the new center for $i$.
We then assign all observations to their closest centers (including the proposed new center for $i$) and compute the within-cluster sum-of-squares (WCSS).
If the WCSS is lower than that with the original center for $i$, we accept this change to $i$'s center; otherwise we continue to use the existing center.
This is repeated for all clusters to obtain an updated set of centers that is refined with the usual methods, e.g., Hartigan-Wong \cite{hartigan1979algorithm} or Lloyd.

Algorithm \ref{algo:main} contains a more specific description of the reinitialization procedure.
Here, $\mathcal{C}$ contains the set of $k$ existing centers while $\mathcal{V}$ contains a set of $n$ observations.
We compute the WCSS before ($w$) and after ($\tilde{w}$) the proposed change and accept the proposal if the latter is smaller.
(Note that the centers are used as-is in the WCSS calculation - for simplicity, we do not recompute them from the observations assigned to each cluster.)
We also permit multiple attempts to sample a $p$ that may achieve a lower WCSS.
The algorithm does not make any distinction between the original and new observations - 
both are treated equally, given that the locations of the existing centers are the only information that is retained from the original clustering. 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}
\caption{Pseudocode for k-means reinitialization.}
\begin{algorithmic} 
\REQUIRE $\mathcal{C} = \{\mathbf{c}_1, \mathbf{c}_2, \ldots, \mathbf{c}_k \}$, $\mathcal{V} = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \}$
\ENSURE An updated set of centers in $\mathcal{C}$.
\FOR{$i \leftarrow 1$ \TO $k$}
\STATE{
    Define $\mathbf{d} \leftarrow \{d_1, d_2, \ldots, d_n\}$
    \STATE{$w \leftarrow 0$}
    \FOR{$j \leftarrow 1$ \TO $n$}
    \STATE{
        $d_j \leftarrow \min_{\mathbf{c} \in \mathcal{C}} || \mathbf{c} - \mathbf{v}_j ||^2$
        \STATE{$w \leftarrow w + d_j$}
    }
    \ENDFOR

    \FOR{$a \leftarrow 1$ \TO $attempts$}
    \STATE{
        Sample $p$ from $\{1, 2, \ldots, n\}$ with probability vector $\mathbf{d}$
        \STATE{$\tilde{w} \leftarrow 0$}
        \FOR{$j \leftarrow 1$ \TO $n$}
        \STATE{$\tilde{w} \leftarrow \tilde{w} + \min (d_j, || \mathbf{v}_p - \mathbf{v}_j ||^2)$}
        \ENDFOR
        \IF{$w > \tilde{w}$} 
            \STATE{
                $\mathbf{c}_i \leftarrow \mathbf{v}_p$ \\
                \textbf{break}
            }
        \ENDIF
    }
    \ENDFOR
}
\ENDFOR
\end{algorithmic}
\label{algo:main}
\end{algorithm}

The distance-weighted sampling means that new centers are proposed at observations that are far away from existing centers.
This encourages the creation of new centers inside distinct subpopulations that were not present in the original dataset, allowing us to achieve a large WCSS decrease for the new observations.
The procedure also prefers to relocate centers for weakly separated clusters as their observations can be reassigned to a neighboring cluster without a large WCSS increase.
However, in the absence of any new subpopulations, the existing center is implicitly favored.
Having been through several iterations of refinement during the original clustering, an existing center should achieve a lower WCSS than a relocated center at a random observation.

The number of attempts dictates the effort taken to search for an acceptable update.
A greater number of attempts increases the probability of capturing new subpopulations, as we are more likely to propose centers using observations from those subpopulations.
However, this comes at the cost of some more computational work.
We perform 10 attempts by default, though the exact choice does not seem to have a major impact on the results.

Reinitialization runs in $O(nk\log k)$ time where $\log k$ is the time complexity of a vantage point tree-based search \cite{yianilos1993data} to find the closest center.
An implementation can reuse the per-observation identities of the closest centers across iterations to reduce the computational cost.
We can also effectively parallelize the calculations across observations using frameworks like OpenMP.

\section{Evaluation on simulated data}

\subsection{Overview}

Each simulation scenario involves an original dataset plus a new batch of observations.
We create a clustering on the original dataset by performing kmeans++ initialization \cite{vassilvitskii2006kmeanspp} followed by 10 iterations of Hartigan-Wong refinement.
We then add the new batch of observations to create the expanded dataset.
With the reinitialization strategy, we use the method described in Algorithm \ref{algo:main} to update the cluster centers.
These centers are used in another 10 Hartigan-Wong iterations on the expanded dataset to obtain a new clustering.
With the simple method, we use the existing cluster centers in the Hartigan-Wong iterations on the expanded dataset without any alteration.
Finally, with the fresh clustering, we simply repeat the initialization and refinement without using any information from the original clustering.

We use two metrics to evaluate the performance of each strategy.
We quantify the quality of the clustering by computing the WCSS of the new clustering in the expanded dataset.
We also compute the adjusted Rand index (ARI) between the original and new clusterings for the subset of observations in the original dataset.
This captures the similarity in the clusterings after the addition of the new batch.

\subsection{Replicate batches}
\label{sec:repbatch}

We use a simple simulation framework involving 1000 observations that are randomly distributed among $X$ distinct subpopulations in 5-dimensional space.
The center $\mathbf{s}_x$ for subpopulation $x$ is sampled from a $\mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$ distribution,
while the location for each observation in $x$ is sampled from a $\mathcal{N}(\mathbf{s}_x, \mathbf{I})$ distribution.
We randomly put a proportion $Y$ of observations into the new batch, using the remaining observations as the original dataset.
In effect, the original dataset and the new batch are replicates of each other as they arise from the same sampling procedure.

\subsection{Batch-specific subpopulations}

We re-use the same framework from Section \ref{sec:repbatch}, but this time, we randomly select $Z$ subpopulations to define the new batch.
All observations in the selected subpopulations are excluded from the original dataset, meaning that the new batch will contain entirely unique subpopulations.

We observe that the reinitialization strategy regularly achieves the lowest WCSS across scenarios (Table \ref{tab:holdout-subpop}), consistent with its ability to adapt to new subpopulations.
In fact, the WCSS from reinitialization is often lower than that of a fresh clustering, presumably because the former has a better starting point for refinement.
The most obvious differences occur when $\sigma$ is large and $Z$ is a large proportion of $X$, such that any failure to form clusters at the new subpopulations inflates the WCSS.
Reinitialization also achieves higher ARIs compared to a fresh clustering, which is most pronounced when $k$ is larger than $X$;
this causes the formation of arbitrary partitions within subpopulations that are difficult to recapitulate without any existing information.
While the ARIs are lower than those of the simple method, this is acceptable given that an existing center needs to be reassigned to a new subpopulation.

\begin{table}
\caption{Differences in the WCSS and ARI for the simple and fresh clustering strategies compared to reinitialization for a range of simulation scenarios.
The percentage increase in the WCSS and the absolute increase in the ARI are reported along with the standard error across 100 iterations.}
\label{tab:holdout-subpop}
\begin{center}
\begin{tabular}{r r r r r r r r}
\hline
$X$ & $\sigma$ & $Z$ & $k$ & WCSS (simple) & WCSS (fresh) & ARI (simple) & ARI (fresh) \\
\hline
5 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.01$ & $-0.06 \pm 0.01$ \\
10 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.14 \pm 0.02$ \\
20 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.27 \pm 0.02$ \\
5 & 5 & 1 & 5 & $129 \pm 15$ & $53 \pm 11$ & $0.04 \pm 0.01$ & $-0.08 \pm 0.01$ \\
10 & 5 & 1 & 5 & $6 \pm 1$ & $6 \pm 2$ & $0.07 \pm 0.01$ & $-0.19 \pm 0.02$ \\
20 & 5 & 1 & 5 & $1 \pm 0$ & $2 \pm 1$ & $0.01 \pm 0.01$ & $-0.34 \pm 0.02$ \\
5 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.01$ & $-0.01 \pm 0.01$ \\
10 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.01$ & $-0.07 \pm 0.01$ \\
20 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.20 \pm 0.02$ \\
5 & 5 & 3 & 5 & $192 \pm 20$ & $38 \pm 10$ & $0.12 \pm 0.01$ & $0.01 \pm 0.01$ \\
10 & 5 & 3 & 5 & $7 \pm 1$ & $7 \pm 1$ & $0.08 \pm 0.02$ & $-0.11 \pm 0.02$ \\
20 & 5 & 3 & 5 & $1 \pm 0$ & $3 \pm 1$ & $0.03 \pm 0.01$ & $-0.27 \pm 0.02$ \\
5 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.21 \pm 0.01$ \\
10 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.27 \pm 0.01$ \\
20 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.32 \pm 0.01$ \\
5 & 5 & 1 & 10 & $4 \pm 2$ & $0 \pm 0$ & $0.14 \pm 0.01$ & $-0.17 \pm 0.01$ \\
10 & 5 & 1 & 10 & $58 \pm 5$ & $43 \pm 6$ & $0.06 \pm 0.01$ & $-0.07 \pm 0.01$ \\
20 & 5 & 1 & 10 & $6 \pm 1$ & $6 \pm 1$ & $0.03 \pm 0.01$ & $-0.24 \pm 0.01$ \\
5 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.05 \pm 0.01$ & $-0.02 \pm 0.01$ \\
10 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.09 \pm 0.01$ \\
20 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.22 \pm 0.01$ \\
5 & 5 & 3 & 10 & $45 \pm 11$ & $0 \pm 0$ & $0.26 \pm 0.01$ & $0.03 \pm 0.01$ \\
10 & 5 & 3 & 10 & $98 \pm 10$ & $31 \pm 4$ & $0.04 \pm 0.01$ & $-0.05 \pm 0.01$ \\
20 & 5 & 3 & 10 & $9 \pm 1$ & $8 \pm 1$ & $0.06 \pm 0.01$ & $-0.16 \pm 0.01$ \\
5 & 1 & 1 & 20 & $1 \pm 0$ & $0 \pm 0$ & $0.05 \pm 0.01$ & $-0.24 \pm 0.01$ \\
10 & 1 & 1 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.34 \pm 0.01$ \\
20 & 1 & 1 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.00$ & $-0.42 \pm 0.01$ \\
5 & 5 & 1 & 20 & $4 \pm 0$ & $1 \pm 0$ & $0.22 \pm 0.01$ & $-0.17 \pm 0.01$ \\
10 & 5 & 1 & 20 & $9 \pm 3$ & $1 \pm 0$ & $0.07 \pm 0.01$ & $-0.24 \pm 0.01$ \\
20 & 5 & 1 & 20 & $37 \pm 3$ & $35 \pm 3$ & $0.08 \pm 0.01$ & $-0.07 \pm 0.01$ \\
5 & 1 & 3 & 20 & $1 \pm 0$ & $0 \pm 0$ & $0.06 \pm 0.01$ & $-0.02 \pm 0.01$ \\
10 & 1 & 3 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.02 \pm 0.01$ & $-0.19 \pm 0.01$ \\
20 & 1 & 3 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.01$ & $-0.31 \pm 0.01$ \\
5 & 5 & 3 & 20 & $31 \pm 9$ & $1 \pm 0$ & $0.36 \pm 0.01$ & $0.04 \pm 0.01$ \\
10 & 5 & 3 & 20 & $16 \pm 4$ & $1 \pm 0$ & $0.16 \pm 0.01$ & $-0.08 \pm 0.01$ \\
20 & 5 & 3 & 20 & $48 \pm 3$ & $33 \pm 3$ & $0.06 \pm 0.01$ & $-0.07 \pm 0.01$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Evaluation on real data}

\bibliography{ref.bib}
\bibliographystyle{plain}

\end{document}
