\documentclass{article}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Re-initializing k-means clustering with additional observations}
\author[1]{Aaron Lun}
\affil[1]{Genentech, Inc. South San Francisco, CA}

\begin{document}
\maketitle

\section{Motivation}

We consider the problem of k-means clustering \cite{lloyd1982least} after a batch of extra observations have been added to the original dataset.
More specifically, assume that the original dataset has already been subjected to k-means clustering.
After adding the extra observations, we wish to obtain a new clustering of the expanded dataset with the same $k$.
One obvious approach is to just assign each new observation to the closest existing cluster followed by post-initialization refinement - we will refer to this as the simple method.
The other obvious approach is to repeat the entire k-means procedure on the expanded dataset without using any information from the existing clustering - we will refer to this as a "fresh" clustering.

We would like our new clustering to account for new subpopulations among the recently added observations.
This ensures that our new clustering can capture subpopulations that were not observed in the original dataset,
which is useful when observations are coming in batches from multiple non-replicate sources.
Unfortunately, the simple method struggles with this requirement as it starts in a local minimum based on the existing cluster centers.
To escape, a new subpopulation must be large enough to relocate the center of the closest existing cluster, 
while hoping that the observations for that existing cluster can be sensibly reassigned to another existing cluster.

We would also like our new clustering to retain some similarity to the existing clustering of the original dataset.
This is useful when the existing clustering is known to be of interest - 
for example, a domain expert has manually generated annotations for each cluster,
and we want to preserve as much of this information as we can in our new clustering.
It can also be desirable when the existing clustering is high-quality but expensive, e.g., after long iteration times or multiple restarts,
and we would like to maintain quality without repeating the computational work.
Such preservation is difficult to achieve in a fresh clustering where non-obvious partitions may not be recapitulated.

\section{Proposed re-initialization algorithm}

To address these two concerns, we developed a algorithm for ``re-initialization" of a k-means clustering in an expanded dataset containing a batch of new observations.
For each existing cluster $i$, we imagine a scenario where $i$'s center is removed from the set of cluster centers.
We compute the squared distance of each observation to its closest remaining center.
We randomly sample a single observation where the sampling probability for each observation is proportional to its squared distance \cite{vassilvitskii2006kmeanspp}.
The sampled observation is our proposal for the new center for $i$.
We then assign all observations to their closest centers (including the proposed new center for $i$) and compute the within-cluster sum-of-squares (WCSS).
If the WCSS is lower than that with the original center for $i$, we accept this change to $i$'s center; otherwise we continue to use the existing center.
This is repeated for all clusters to obtain an updated set of centers that is refined with the usual methods, e.g., Hartigan-Wong \cite{hartigan1979algorithm} or Lloyd.

Algorithm \ref{algo:main} contains a more specific description of the reinitialization procedure.
Here, $\mathcal{C}$ contains the set of $k$ existing centers while $\mathcal{V}$ contains a set of $n$ observations.
We compute the WCSS before ($w$) and after ($\tilde{w}$) the proposed change and accept the proposal if the latter is smaller.
(Note that the centers are used as-is in the WCSS calculation - for simplicity, we do not recompute them from the observations assigned to each cluster.)
We also permit multiple attempts to sample a $p$ that may achieve a lower WCSS.
The algorithm does not make any distinction between the original and new observations - 
both are treated equally, given that the locations of the existing centers are the only information that is retained from the original clustering. 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}
\caption{Pseudocode for k-means reinitialization.}
\begin{algorithmic} 
\REQUIRE $\mathcal{C} = \{\mathbf{c}_1, \mathbf{c}_2, \ldots, \mathbf{c}_k \}$, $\mathcal{V} = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \}$
\ENSURE An updated set of centers in $\mathcal{C}$.
\FOR{$i \leftarrow 1$ \TO $k$}
\STATE{
    Define $\mathbf{d} \leftarrow \{d_1, d_2, \ldots, d_n\}$
    \STATE{$w \leftarrow 0$}
    \FOR{$j \leftarrow 1$ \TO $n$}
    \STATE{
        $d_j \leftarrow \min_{\mathbf{c} \in \mathcal{C}} || \mathbf{c} - \mathbf{v}_j ||^2$
        \STATE{$w \leftarrow w + d_j$}
    }
    \ENDFOR

    \FOR{$a \leftarrow 1$ \TO $attempts$}
    \STATE{
        Sample $p$ from $\{1, 2, \ldots, n\}$ with probability vector $\mathbf{d}$
        \STATE{$\tilde{w} \leftarrow 0$}
        \FOR{$j \leftarrow 1$ \TO $n$}
        \STATE{$\tilde{w} \leftarrow \tilde{w} + \min (d_j, || \mathbf{v}_p - \mathbf{v}_j ||^2)$}
        \ENDFOR
        \IF{$w > \tilde{w}$} 
            \STATE{
                $\mathbf{c}_i \leftarrow \mathbf{v}_p$ \\
                \textbf{break}
            }
        \ENDIF
    }
    \ENDFOR
}
\ENDFOR
\end{algorithmic}
\label{algo:main}
\end{algorithm}

The distance-weighted sampling means that new centers are proposed at observations that are far away from existing centers.
This encourages the creation of new centers inside distinct subpopulations that were not present in the original dataset, allowing us to achieve a large WCSS decrease for the new observations.
The procedure also prefers to relocate centers for weakly separated clusters as their observations can be reassigned to a neighboring cluster without a large WCSS increase.
However, in the absence of any new subpopulations, the existing center is implicitly favored.
Having been through several iterations of refinement during the original clustering, an existing center should achieve a lower WCSS than a relocated center at a random observation.

The number of attempts dictates the effort taken to search for an acceptable update.
A greater number of attempts increases the probability of capturing new subpopulations, as we are more likely to propose centers using observations from those subpopulations.
However, this comes at the cost of some more computational work.
We perform 10 attempts by default, though the exact choice does not seem to have a major impact on the results.

Reinitialization runs in $O(nk\log k)$ time where $\log k$ is the time complexity of a vantage point tree-based search \cite{yianilos1993data} to find the closest center.
An implementation can reuse the per-observation identities of the closest centers across iterations to reduce the computational cost.
We can also effectively parallelize the calculations across observations using frameworks like OpenMP.

\section{Evaluation on simulated data}

\subsection{Overview}

Each simulation scenario involves the creation of one set of observations representing the original dataset and another set representing the new batch.
We cluster the original dataset by performing kmeans++ initialization \cite{vassilvitskii2006kmeanspp} followed by 10 iterations of Hartigan-Wong refinement.
We then add the new batch of observations to create the expanded dataset that is subjected to further clustering with some of the previously described methods.
For reinitialization, we use an implementation of Algorithm \ref{algo:main} to update the cluster centers.
These centers are used in another 10 Hartigan-Wong iterations on the expanded dataset to obtain a new clustering.
For the simple method, we directly use the existing cluster centers in the Hartigan-Wong iterations on the expanded dataset without any alteration.
Finally, for the fresh clustering, we repeat the initialization and refinement without using any values from the original clustering.

We use two metrics to evaluate the performance of each strategy in each simulation scenario.
We quantify the quality of the clustering by computing the WCSS of the new clustering in the expanded dataset.
We also compute the adjusted Rand index (ARI) between the original and new clusterings for the subset of observations in the original dataset.
This represents the relative similarity in the clusterings after the addition of the new batch.

\subsection{Replicate batches}
\label{sec:repbatch}

We start with a simple simulation framework involving 1000 observations that are randomly distributed among $X$ distinct subpopulations in 5-dimensional space.
The center $\mathbf{s}_x$ for subpopulation $x$ is sampled from a $\mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$ distribution,
while the location for each observation in $x$ is sampled from a $\mathcal{N}(\mathbf{s}_x, \mathbf{I})$ distribution.
We randomly select a proportion $Y$ of observations and put them into the new batch; the remaining observations are used as the original dataset.
In effect, the original dataset and the new batch are replicates of each other as they arise from the same sampling procedure.

For these scenarios, the reinitialization strategy achieves the lowest WCSS and close to the highest ARI (Table \ref{tab:replicate}).
We see the greatest difference in the WCSS when $k$ is close to $X$, likely due to a poor initialization in the original or fresh clusterings;
in contrast, reinitialization has a chance to overcome a poor start by only accepting updates that decrease the WCSS.
ARIs are highest for the simple method, consistent with its complete reliance on the existing cluster centers.
Nonetheless, the ARIs from reinitialization are close to the maximum and are consistently larger than those from a fresh clustering, as expected.

\begin{table}
\caption{Differences in the WCSS and ARI for the simple and fresh clustering strategies compared to reinitialization for a range of simulations with replicate batches.
The percentage increase in the WCSS and the absolute increase in the ARI are reported along with the standard error across 100 iterations.}
\label{tab:replicate}
\begin{center}
\begin{tabular}{r r r r r r r r}
\hline
$X$ & $\sigma$ & $Y$ & $k$ & WCSS (simple) & WCSS (fresh) & ARI (simple) & ARI (fresh) \\
\hline
5 & 1 & 0.1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.14 \pm 0.02$ \\
10 & 1 & 0.1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.20 \pm 0.02$ \\
5 & 5 & 0.1 & 5 & $32 \pm 8$ & $34 \pm 6$ & $0.05 \pm 0.01$ & $-0.06 \pm 0.01$ \\
10 & 5 & 0.1 & 5 & $6 \pm 1$ & $6 \pm 2$ & $0.04 \pm 0.01$ & $-0.26 \pm 0.02$ \\
5 & 1 & 0.5 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.08 \pm 0.01$ \\
10 & 1 & 0.5 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.15 \pm 0.02$ \\
5 & 5 & 0.5 & 5 & $42 \pm 10$ & $44 \pm 9$ & $0.07 \pm 0.01$ & $-0.05 \pm 0.01$ \\
10 & 5 & 0.5 & 5 & $2 \pm 1$ & $4 \pm 1$ & $0.02 \pm 0.01$ & $-0.28 \pm 0.02$ \\
5 & 1 & 0.1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.42 \pm 0.01$ \\
10 & 1 & 0.1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.31 \pm 0.01$ \\
5 & 5 & 0.1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.32 \pm 0.01$ \\
10 & 5 & 0.1 & 10 & $38 \pm 4$ & $47 \pm 5$ & $0.10 \pm 0.01$ & $-0.06 \pm 0.01$ \\
5 & 1 & 0.5 & 10 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.27 \pm 0.01$ \\
10 & 1 & 0.5 & 10 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.20 \pm 0.01$ \\
5 & 5 & 0.5 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.00$ & $-0.27 \pm 0.01$ \\
10 & 5 & 0.5 & 10 & $43 \pm 5$ & $34 \pm 4$ & $0.10 \pm 0.01$ & $-0.04 \pm 0.01$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Batch-specific subpopulations}

We re-use the same framework from Section \ref{sec:repbatch}, but this time, we randomly select $Z$ subpopulations to define the new batch.
All observations in the selected subpopulations are excluded from the original dataset, meaning that the new batch will contain entirely unique subpopulations.

We observe that the reinitialization strategy regularly achieves the lowest WCSS across scenarios (Table \ref{tab:holdout-subpop}), consistent with its ability to adapt to new subpopulations.
The most obvious differences occur when $\sigma$ is large and $Z$ is a large proportion of $X$, such that any failure to form distinct clusters at the new subpopulations inflates the WCSS.
(The effect of $k \approx X$ seen in Section \ref{sec:repbatch} is also relevant here.)
Reinitialization achieves higher ARIs compared to a fresh clustering, which is most pronounced when $k$ is larger than $X$;
this causes the formation of arbitrary partitions within subpopulations that are difficult to recapitulate without any existing information.
While the ARIs are lower than those of the simple method, this is arguably acceptable given that an existing center needs to be reassigned to a new subpopulation.

\begin{table}
\caption{Differences in the WCSS and ARI for the simple and fresh clustering strategies compared to reinitialization for a range of simulations involving batch-specific subpopulations. 
The percentage increase in the WCSS and the absolute increase in the ARI are reported along with the standard error across 100 iterations.}
\label{tab:holdout-subpop}
\begin{center}
\begin{tabular}{r r r r r r r r}
\hline
$X$ & $\sigma$ & $Z$ & $k$ & WCSS (simple) & WCSS (fresh) & ARI (simple) & ARI (fresh) \\
\hline
5 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.01$ & $-0.06 \pm 0.01$ \\
10 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.14 \pm 0.02$ \\
20 & 1 & 1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.27 \pm 0.02$ \\
5 & 5 & 1 & 5 & $129 \pm 15$ & $53 \pm 11$ & $0.04 \pm 0.01$ & $-0.08 \pm 0.01$ \\
10 & 5 & 1 & 5 & $6 \pm 1$ & $6 \pm 2$ & $0.07 \pm 0.01$ & $-0.19 \pm 0.02$ \\
20 & 5 & 1 & 5 & $1 \pm 0$ & $2 \pm 1$ & $0.01 \pm 0.01$ & $-0.34 \pm 0.02$ \\
5 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.01$ & $-0.01 \pm 0.01$ \\
10 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.01$ & $-0.07 \pm 0.01$ \\
20 & 1 & 3 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.20 \pm 0.02$ \\
5 & 5 & 3 & 5 & $192 \pm 20$ & $38 \pm 10$ & $0.12 \pm 0.01$ & $0.01 \pm 0.01$ \\
10 & 5 & 3 & 5 & $7 \pm 1$ & $7 \pm 1$ & $0.08 \pm 0.02$ & $-0.11 \pm 0.02$ \\
20 & 5 & 3 & 5 & $1 \pm 0$ & $3 \pm 1$ & $0.03 \pm 0.01$ & $-0.27 \pm 0.02$ \\
5 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.21 \pm 0.01$ \\
10 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.27 \pm 0.01$ \\
20 & 1 & 1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.32 \pm 0.01$ \\
5 & 5 & 1 & 10 & $4 \pm 2$ & $0 \pm 0$ & $0.14 \pm 0.01$ & $-0.17 \pm 0.01$ \\
10 & 5 & 1 & 10 & $58 \pm 5$ & $43 \pm 6$ & $0.06 \pm 0.01$ & $-0.07 \pm 0.01$ \\
20 & 5 & 1 & 10 & $6 \pm 1$ & $6 \pm 1$ & $0.03 \pm 0.01$ & $-0.24 \pm 0.01$ \\
5 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.05 \pm 0.01$ & $-0.02 \pm 0.01$ \\
10 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.09 \pm 0.01$ \\
20 & 1 & 3 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.00$ & $-0.22 \pm 0.01$ \\
5 & 5 & 3 & 10 & $45 \pm 11$ & $0 \pm 0$ & $0.26 \pm 0.01$ & $0.03 \pm 0.01$ \\
10 & 5 & 3 & 10 & $98 \pm 10$ & $31 \pm 4$ & $0.04 \pm 0.01$ & $-0.05 \pm 0.01$ \\
20 & 5 & 3 & 10 & $9 \pm 1$ & $8 \pm 1$ & $0.06 \pm 0.01$ & $-0.16 \pm 0.01$ \\
5 & 1 & 1 & 20 & $1 \pm 0$ & $0 \pm 0$ & $0.05 \pm 0.01$ & $-0.24 \pm 0.01$ \\
10 & 1 & 1 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $-0.34 \pm 0.01$ \\
20 & 1 & 1 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.00$ & $-0.42 \pm 0.01$ \\
5 & 5 & 1 & 20 & $4 \pm 0$ & $1 \pm 0$ & $0.22 \pm 0.01$ & $-0.17 \pm 0.01$ \\
10 & 5 & 1 & 20 & $9 \pm 3$ & $1 \pm 0$ & $0.07 \pm 0.01$ & $-0.24 \pm 0.01$ \\
20 & 5 & 1 & 20 & $37 \pm 3$ & $35 \pm 3$ & $0.08 \pm 0.01$ & $-0.07 \pm 0.01$ \\
5 & 1 & 3 & 20 & $1 \pm 0$ & $0 \pm 0$ & $0.06 \pm 0.01$ & $-0.02 \pm 0.01$ \\
10 & 1 & 3 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.02 \pm 0.01$ & $-0.19 \pm 0.01$ \\
20 & 1 & 3 & 20 & $0 \pm 0$ & $0 \pm 0$ & $0.00 \pm 0.01$ & $-0.31 \pm 0.01$ \\
5 & 5 & 3 & 20 & $31 \pm 9$ & $1 \pm 0$ & $0.36 \pm 0.01$ & $0.04 \pm 0.01$ \\
10 & 5 & 3 & 20 & $16 \pm 4$ & $1 \pm 0$ & $0.16 \pm 0.01$ & $-0.08 \pm 0.01$ \\
20 & 5 & 3 & 20 & $48 \pm 3$ & $33 \pm 3$ & $0.06 \pm 0.01$ & $-0.07 \pm 0.01$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Continuous trajectories}

Finally, we simulate points lying in a circle on a 2-dimensional plane with random $\mathcal{N}(0, \sigma^2\mathbf{I})$ noise.
To create the new batch, we take all cells in an arc of the circle corresponding to a proportion $U$ of the circumference.
This represents a continuous trajectory of new observations rather than discrete subpopulations.

WCSS values are comparable for all methods while the highest ARIs are shared between reinitialization and the simple method (Table \ref{tab:circular}).
The fresh clustering has the lowest ARI, which is expected given the absence of a clear partitioning in a circular mass of observations.
The low WCSS for the simple method is due to the proximity of the new and existing observations,
allowing the centers to incrementally migrate into the new batch during refinement.
This is more difficult in scenarios involving new subpopulations where the simple method is more likely to be trapped in a local minimum.

\begin{table}
\caption{Differences in the WCSS and ARI for the simple and fresh clustering strategies compared to reinitialization for a range of simulations involving a circular trajectory.
The percentage increase in the WCSS and the absolute increase in the ARI are reported along with the standard error across 100 iterations.}
\label{tab:circular}
\begin{center}
\begin{tabular}{r r r r r r r}
\hline
$\sigma$ & $A$ & $k$ & WCSS (simple) & WCSS (fresh) & ARI (simple) & ARI (fresh) \\
\hline
0.1 & 0.1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.02 \pm 0.01$ & $-0.17 \pm 0.02$ \\
0.5 & 0.1 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.00 \pm 0.00$ & $-0.21 \pm 0.02$ \\
0.1 & 0.25 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.04 \pm 0.01$ & $-0.02 \pm 0.01$ \\
0.5 & 0.25 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.01$ & $-0.08 \pm 0.01$ \\
0.1 & 0.5 & 5 & $0 \pm 0$ & $0 \pm 0$ & $-0.01 \pm 0.01$ & $-0.01 \pm 0.01$ \\
0.5 & 0.5 & 5 & $0 \pm 0$ & $0 \pm 0$ & $0.02 \pm 0.01$ & $0.00 \pm 0.01$ \\
0.1 & 0.1 & 10 & $1 \pm 0$ & $1 \pm 0$ & $0.01 \pm 0.01$ & $-0.10 \pm 0.01$ \\
0.5 & 0.1 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.01 \pm 0.01$ & $-0.30 \pm 0.01$ \\
0.1 & 0.25 & 10 & $1 \pm 0$ & $0 \pm 0$ & $-0.01 \pm 0.01$ & $-0.02 \pm 0.01$ \\
0.5 & 0.25 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.02 \pm 0.01$ & $-0.12 \pm 0.01$ \\
0.1 & 0.5 & 10 & $0 \pm 0$ & $0 \pm 0$ & $0.03 \pm 0.01$ & $0.01 \pm 0.01$ \\
0.5 & 0.5 & 10 & $0 \pm 0$ & $0 \pm 0$ & $-0.01 \pm 0.01$ & $-0.02 \pm 0.01$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Evaluation on real data}

\bibliography{ref.bib}
\bibliographystyle{plain}

\end{document}
